<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on mak&#39;s blog</title>
    <link>https://xymak.github.io/blog/posts/</link>
    <description>Recent content in Posts on mak&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 May 2020 16:14:54 +0800</lastBuildDate>
    
	<atom:link href="https://xymak.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：Channel</title>
      <link>https://xymak.github.io/blog/posts/golang-channel/</link>
      <pubDate>Thu, 28 May 2020 16:14:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-channel/</guid>
      <description>今天看了《Go 语言设计与实现》Channel相关的介绍。当初有遇到相关的问题，答得不是很好，这里记录一下。 其实golang channel的设计还是很简单的，结构体如下：
type hchan struct { qcount uint dataqsiz uint buf unsafe.Pointer elemsize uint16 closed uint32 elemtype *_type sendx uint recvx uint recvq waitq sendq waitq lock mutex }  qcount为当前元素个数 dataqsiz为总容量 buf为缓冲区数据指针 sendx为发送的标记位置 recvx为接收的标记位置 sendq为缓冲区不足的情况下阻塞的goroutine队列 recvq为不存在数据的情况下阻塞的goroutine队列  大体流程是这样的：
 发送情况：  如果是缓冲区空间不足或者不存在缓冲区，这里有两种情况：  有goroutine在接收队列等待，会直接将数据拷贝到接收goroutine的变量里面，sendq出队，并且唤醒接收goroutine 没有等待的goroutine，则通过gopark将当前goroutine设置为wait，并且初始化一个sudog入队recvq   如果缓存区空间充足：  有goroutine在接收队列等待，会直接将数据拷贝到接收goroutine的变量里面，sendq出队，并且唤醒接收goroutine 没有等待的goroutine，则写入buf     接收情况： 跟发送类似，镜像操作，这里不做多余陈述  另外，从底层也能发现，channel里面只有一个锁，接收和发送都要通过这一个锁做并发控制，所以用channel替代一些无锁的写法，性能是很差的。</description>
    </item>
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：SingleFlight</title>
      <link>https://xymak.github.io/blog/posts/golang-single-flight/</link>
      <pubDate>Mon, 25 May 2020 20:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-single-flight/</guid>
      <description>今天读《Go 语言设计与实现》看到了一个感觉比较有用的golang同步原语，它能在一个服务中抑制多次重复的请求。
&amp;ldquo;一个常用的场景是 - 我们使用redis对数据库中的数据进行缓存，发生缓存击穿，大量流量都会达到数据上进而影响服务的延时&amp;rdquo;，这句话是作者的原话。从这句话我可以延伸到，还可以用来做合并请求，例如cdn上的合并请求。 之前我还因为缓存击穿扛过一个事故，就是mysql请求一直超时，redis缓存一直打不上，造成了恶性循环，所以我觉得这个同步原语非常有用。
它的结构体 x/sync/singleflight.Group 由一个映射表和一个互斥锁组成：
type Group struct { mu sync.Mutex m map[string]*call } type call struct { wg sync.WaitGroup val interface{} err error dups int chans []chan&amp;lt;- Result } 它对外提供了两个用于抑制请求的方法：
 x/sync/singleflight.Group.Do — 同步等待的方法 Do； x/sync/singleflight.Group.DoChan — 返回 Channel 异步等待的方法；  func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.</description>
    </item>
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：互斥锁</title>
      <link>https://xymak.github.io/blog/posts/golang-mutex/</link>
      <pubDate>Mon, 25 May 2020 00:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-mutex/</guid>
      <description>最近在读《Go 语言设计与实现》的时候，里面提到golang里的互斥锁分正常模式和饥饿模式，饥饿模式是golang 1.9版本引入的优化。饥饿这个次有点抽象，我调研了一下为什么做了这方面的优化。 下面引入一下YiXu Zhang 写的文章
文章里的例子：
func main() { done := make(chan bool, 1) var mu sync.Mutex // goroutine 1  go func() { for { select { case &amp;lt;-done: return default: mu.Lock() time.Sleep(100 * time.Microsecond) mu.Unlock() } } }() // goroutine 2  for i := 0; i &amp;lt; 10; i++ { time.Sleep(100 * time.Microsecond) mu.Lock() mu.Unlock() } done &amp;lt;- true } 如果用golang 1.8运行上面这个例子:
Lock acquired per goroutine: g1: 7200216 g2: 10 互斥锁被第二个协程捕获十次，而第一个协程则捕获了700万次。首先协程1获得锁后睡眠100ms，当协程2尝试获取锁时，它将被添加到锁的队列（FIFO）中，并且进入等待状态。 然后，协程1释放了锁。协程2也被唤醒了，被标记为可运行，然后等待调度器在线程上运行。 但是这个时候协程1又去获得锁。 到协程2去获得锁的时候，发现锁已经被其他线程持有了。 这个问题可以总结为：刚被唤起的协程与在运行的协程竞争时，大概率会获取不到锁。 本身golang 1.</description>
    </item>
    
    <item>
      <title>利用gdb分析nginx core dump文件</title>
      <link>https://xymak.github.io/blog/posts/nginx-core-dump-gdb/</link>
      <pubDate>Sat, 02 May 2020 17:50:57 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/nginx-core-dump-gdb/</guid>
      <description>今天学习极客时间《Nginx核心知识100讲》，其中介绍了gdb怎么分析nginx的core dump文件。我在这里记录下来。
首先在nginx.conf中加入core dump文件（核心转储文件）的配置，保证nginx对目录有写权限
worker_rlimit_core 90m; working_directory /tmp/nginxcore/; 然后给nginx master发个SIGHUP信号。
kill -1 ${PID} 给其中有一个worker发送SIGSEGV信号，使nginx生成core dump文件.
kill -s SIGSEGV ${PID} 然后通过gdb分析现场。
gdb nginx core GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;lt;http://gnu.org/licenses/gpl.html&amp;gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &amp;quot;show copying&amp;quot; and &amp;quot;show warranty&amp;quot; for details.</description>
    </item>
    
    <item>
      <title>今年晋升答辩的时候遇到的问题：etcd怎么添加新节点？</title>
      <link>https://xymak.github.io/blog/posts/etcd/</link>
      <pubDate>Sat, 18 Apr 2020 00:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/etcd/</guid>
      <description>我自己搭的etcd集群的时候，随便在网上找了个demo就搞定了，没关注这方面的问题。被评为问到就蒙了。今天我遇到的问题都复盘一遍。
1.etcd怎么添加一个节点？ 我之前没搭建过zookeeper、consul之类的集群，所有没关注过。 被问到这个问题，我当时回答是改配置文件，其实是不行的。因为，因为重新启动启动各个节点的时候，会有一致性问题: 需要重启，如果让数据版本比较旧的节点成为了leader，处理了写入请求后，当其他节点重新加入后，这个节点成为最新版本的节点，数据覆盖了之前的版本.
这个时候要通过etcdctl动态添加。加入本身有3个节点：
etcd --name myetcd1 --listen-client-urls http://0.0.0.0:23791 --advertise-client-urls http://0.0.0.0:23791 --listen-peer-urls http://0.0.0.0:23801 --initial-advertise-peer-urls http://0.0.0.0:23801 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 etcd --name myetcd2 --listen-client-urls http://0.0.0.0:23792 --advertise-client-urls http://0.0.0.0:23792 --listen-peer-urls http://0.0.0.0:23802 --initial-advertise-peer-urls http://0.0.0.0:23802 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 etcd --name myetcd3 --listen-client-urls http://0.0.0.0:23793 --advertise-client-urls http://0.0.0.0:23793 --listen-peer-urls http://0.0.0.0:23803 --initial-advertise-peer-urls http://0.0.0.0:23803 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 查看member列表：
etcdctl --endpoints=http://127.0.0.1:23791 member list 47170e1b92eadb07: name=myetcd1 peerURLs=http://0.0.0.0:23801 clientURLs=http://0.0.0.0:23791 isLeader=true d150980031282a6b: name=myetcd2 peerURLs=http://0.0.0.0:23802 clientURLs=http://0.</description>
    </item>
    
  </channel>
</rss>