<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mak&#39;s blog</title>
    <link>https://xymak.github.io/blog/</link>
    <description>Recent content on mak&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Jun 2020 10:44:54 +0800</lastBuildDate>
    
	<atom:link href="https://xymak.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>直播相关技术整理: rtmp</title>
      <link>https://xymak.github.io/blog/posts/rtmp/</link>
      <pubDate>Sat, 20 Jun 2020 10:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/rtmp/</guid>
      <description>rtmp是基于tcp的，下面结合wireshark来整理rtmp与客户端的交互。测试用的是芒果台的这个流：rtmp://58.200.131.2:1935/livetv/
1.握手
 客户端要等收到S1之后才能发送C2 客户端要等收到S2之后才能发送其他信息（控制信息和真实音视频等数据） 服务端要等到收到C0之后发送S1 服务端必须等到收到C1之后才能发送S2 服务端必须等到收到C2之后才能发送其他信息（控制信息和真实音视频等数据） 握手开始于客户端发送C0、C1块。服务器收到C0或C1后发送S0和S1。 当客户端收齐S0和S1后，开始发送C2。当服务器收齐C0和C1后，开始发送S2。 当客户端和服务器分别收到S2和C2后，握手完成。  从wireshark中也可以看到c1、c2和c2发送
2.发送connect指令
从wireshark查看客户端发送connect的到服务端：
连接信息包含了app名称和地址一些信息，这个streamID据说是一个消息的唯一标识。这里面为0说明这个消息是初始的0消息。Chunk stream ID:message会拆分成多个chunk，同一个Chunk Stream ID必然属于同一个Message。
message type id(消息的类型id)：表示实际发送的数据的类型，如8代表音频数据、9代表视频数据。 Format：指的是chunk type。共有4种不同的格式，其中第一种格式字段为0，可以表示其他三种表示的所有数据，但由于其他三种格式是基于对之前chunk的差量化的表示，因此可以更简洁地表示相同的数据，实际使用的时候还是应该采用尽量少的字节表示相同意义的数据。因为type0是表示不同数据，其他是差量，所以可以想象如果搜不到type0的包说明这个流肯定有问题。可以通过“rtmpt.header.format == 0”过滤。
之后是设置确认窗口大小、带宽协议消息 服务器发送用户控制消息中的“流开始”(Stream Begin)消息到客户端。服务器发送命令消息中的“结果”(_result)，通知客户端连接的状态。可以看到链接已经建立成功。
3.建立一个网络流
网络流代表了发送多媒体数据的通道。服务器和客户端之间只能建立一个网络连接，且多个网络流可以复用这一个网络连接。 服务器收到请求后向客户端发送_result()，对创建流的消息进行响应。此时NetStream创建完成。
4.播放
主要过程是客户端把play指令发到服务端，服务端返回一个状态，之后就是返回音频和视频数据。</description>
    </item>
    
    <item>
      <title>tcp粘包/分包整理</title>
      <link>https://xymak.github.io/blog/posts/tcp-stick-package/</link>
      <pubDate>Wed, 03 Jun 2020 08:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/tcp-stick-package/</guid>
      <description>处理tcp层的逻辑的时候就需要处理粘包问题。因为tcp是基于流的，保证有序，但是流里面消息是没有边界的，tcp会对应用消息做分包，所以会有粘包/分包问题。
使用应用层的协议就不需要，例如http，websocket。这些应用层协议是基于tcp，本身解决粘包问题。
 Http  http的header有&amp;quot;Content-Length&amp;quot;这个就设置了应用消息的长度，接收方可以根据这个长度来感知应用消息的长度。
http在&amp;quot;Transfer-Encoding: chunked&amp;quot;场景下是没有Content-Length的。在头部加入 Transfer-Encoding: chunked 之后，就代表这个报文采用了分块编码。这时，报文中的实体需要改为用一系列分块来传输。每个分块包含十六进制的长度值和数据，长度值独占一行，长度不包括它结尾的 CRLF（\r\n），也不包括分块数据结尾的 CRLF。最后一个分块长度值必须为 0，对应的分块数据没有内容，表示实体结束。 例如golang以下代码：
func main() { http.HandleFunc(&amp;#34;/hello&amp;#34;, ChunkServer) http.ListenAndServe(&amp;#34;127.0.0.1:6666&amp;#34;, nil) } func ChunkServer(w http.ResponseWriter, req *http.Request) { flusher, ok := w.(http.Flusher) if !ok { panic(&amp;#34;expected http.ResponseWriter to be an http.Flusher&amp;#34;) } w.Header().Set(&amp;#34;X-Content-Type-Options&amp;#34;, &amp;#34;nosniff&amp;#34;) for i := 1; i &amp;lt;= 10; i++ { fmt.Fprintf(w, &amp;#34;Chunk #%d\n&amp;#34;, i) flusher.Flush() // Trigger &amp;#34;chunked&amp;#34; encoding and send a chunk... 	time.Sleep(500 * time.</description>
    </item>
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：Channel</title>
      <link>https://xymak.github.io/blog/posts/golang-channel/</link>
      <pubDate>Thu, 28 May 2020 16:14:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-channel/</guid>
      <description>今天看了《Go 语言设计与实现》Channel相关的介绍。当初有遇到相关的问题，答得不是很好，这里记录一下。 其实golang channel的设计还是很简单的，结构体如下：
type hchan struct { qcount uint dataqsiz uint buf unsafe.Pointer elemsize uint16 closed uint32 elemtype *_type sendx uint recvx uint recvq waitq sendq waitq lock mutex }  qcount为当前元素个数 dataqsiz为总容量 buf为缓冲区数据指针 sendx为发送的标记位置 recvx为接收的标记位置 sendq为缓冲区不足的情况下阻塞的goroutine队列 recvq为不存在数据的情况下阻塞的goroutine队列  大体流程是这样的：
 发送情况：  如果是缓冲区空间不足或者不存在缓冲区，这里有两种情况：  有goroutine在接收队列等待，会直接将数据拷贝到接收goroutine的变量里面，sendq出队，并且唤醒接收goroutine 没有等待的goroutine，则通过gopark将当前goroutine设置为wait，并且初始化一个sudog入队recvq   如果缓存区空间充足：  有goroutine在接收队列等待，会直接将数据拷贝到接收goroutine的变量里面，sendq出队，并且唤醒接收goroutine 没有等待的goroutine，则写入buf     接收情况： 跟发送类似，镜像操作，这里不做多余陈述  另外，从底层也能发现，channel里面只有一个锁，接收和发送都要通过这一个锁做并发控制，所以用channel替代一些无锁的写法，性能是很差的。</description>
    </item>
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：SingleFlight</title>
      <link>https://xymak.github.io/blog/posts/golang-single-flight/</link>
      <pubDate>Mon, 25 May 2020 20:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-single-flight/</guid>
      <description>今天读《Go 语言设计与实现》看到了一个感觉比较有用的golang同步原语，它能在一个服务中抑制多次重复的请求。
&amp;ldquo;一个常用的场景是 - 我们使用redis对数据库中的数据进行缓存，发生缓存击穿，大量流量都会达到数据上进而影响服务的延时&amp;rdquo;，这句话是作者的原话。从这句话我可以延伸到，还可以用来做合并请求，例如cdn上的合并请求。 之前我还因为缓存击穿扛过一个事故，就是mysql请求一直超时，redis缓存一直打不上，造成了恶性循环，所以我觉得这个同步原语非常有用。
它的结构体 x/sync/singleflight.Group 由一个映射表和一个互斥锁组成：
type Group struct { mu sync.Mutex m map[string]*call } type call struct { wg sync.WaitGroup val interface{} err error dups int chans []chan&amp;lt;- Result } 它对外提供了两个用于抑制请求的方法：
 x/sync/singleflight.Group.Do — 同步等待的方法 Do； x/sync/singleflight.Group.DoChan — 返回 Channel 异步等待的方法；  func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.</description>
    </item>
    
    <item>
      <title>《Go 语言设计与实现》读书笔记：互斥锁</title>
      <link>https://xymak.github.io/blog/posts/golang-mutex/</link>
      <pubDate>Mon, 25 May 2020 00:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/golang-mutex/</guid>
      <description>最近在读《Go 语言设计与实现》的时候，里面提到golang里的互斥锁分正常模式和饥饿模式，饥饿模式是golang 1.9版本引入的优化。饥饿这个次有点抽象，我调研了一下为什么做了这方面的优化。 下面引入一下YiXu Zhang 写的文章
文章里的例子：
func main() { done := make(chan bool, 1) var mu sync.Mutex // goroutine 1  go func() { for { select { case &amp;lt;-done: return default: mu.Lock() time.Sleep(100 * time.Microsecond) mu.Unlock() } } }() // goroutine 2  for i := 0; i &amp;lt; 10; i++ { time.Sleep(100 * time.Microsecond) mu.Lock() mu.Unlock() } done &amp;lt;- true } 如果用golang 1.8运行上面这个例子:
Lock acquired per goroutine: g1: 7200216 g2: 10 互斥锁被第二个协程捕获十次，而第一个协程则捕获了700万次。首先协程1获得锁后睡眠100ms，当协程2尝试获取锁时，它将被添加到锁的队列（FIFO）中，并且进入等待状态。 然后，协程1释放了锁。协程2也被唤醒了，被标记为可运行，然后等待调度器在线程上运行。 但是这个时候协程1又去获得锁。 到协程2去获得锁的时候，发现锁已经被其他线程持有了。 这个问题可以总结为：刚被唤起的协程与在运行的协程竞争时，大概率会获取不到锁。 本身golang 1.</description>
    </item>
    
    <item>
      <title>利用gdb分析nginx core dump文件</title>
      <link>https://xymak.github.io/blog/posts/nginx-core-dump-gdb/</link>
      <pubDate>Sat, 02 May 2020 17:50:57 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/nginx-core-dump-gdb/</guid>
      <description>今天学习极客时间《Nginx核心知识100讲》，其中介绍了gdb怎么分析nginx的core dump文件。我在这里记录下来。
首先在nginx.conf中加入core dump文件（核心转储文件）的配置，保证nginx对目录有写权限
worker_rlimit_core 90m; working_directory /tmp/nginxcore/; 然后给nginx master发个SIGHUP信号。
kill -1 ${PID} 给其中有一个worker发送SIGSEGV信号，使nginx生成core dump文件.
kill -s SIGSEGV ${PID} 然后通过gdb分析现场。
gdb nginx core GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-119.el7 Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &amp;lt;http://gnu.org/licenses/gpl.html&amp;gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &amp;quot;show copying&amp;quot; and &amp;quot;show warranty&amp;quot; for details.</description>
    </item>
    
    <item>
      <title>今年晋升答辩的时候遇到的问题：etcd怎么添加新节点？</title>
      <link>https://xymak.github.io/blog/posts/etcd/</link>
      <pubDate>Sat, 18 Apr 2020 00:44:54 +0800</pubDate>
      
      <guid>https://xymak.github.io/blog/posts/etcd/</guid>
      <description>我自己搭的etcd集群的时候，随便在网上找了个demo就搞定了，没关注这方面的问题。被评为问到就蒙了。今天我遇到的问题都复盘一遍。
1.etcd怎么添加一个节点？ 我之前没搭建过zookeeper、consul之类的集群，所有没关注过。 被问到这个问题，我当时回答是改配置文件，其实是不行的。因为，因为重新启动启动各个节点的时候，会有一致性问题: 需要重启，如果让数据版本比较旧的节点成为了leader，处理了写入请求后，当其他节点重新加入后，这个节点成为最新版本的节点，数据覆盖了之前的版本.
这个时候要通过etcdctl动态添加。加入本身有3个节点：
etcd --name myetcd1 --listen-client-urls http://0.0.0.0:23791 --advertise-client-urls http://0.0.0.0:23791 --listen-peer-urls http://0.0.0.0:23801 --initial-advertise-peer-urls http://0.0.0.0:23801 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 etcd --name myetcd2 --listen-client-urls http://0.0.0.0:23792 --advertise-client-urls http://0.0.0.0:23792 --listen-peer-urls http://0.0.0.0:23802 --initial-advertise-peer-urls http://0.0.0.0:23802 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 etcd --name myetcd3 --listen-client-urls http://0.0.0.0:23793 --advertise-client-urls http://0.0.0.0:23793 --listen-peer-urls http://0.0.0.0:23803 --initial-advertise-peer-urls http://0.0.0.0:23803 --initial-cluster-token etcd-cluster-test --initial-cluster-state new --initial-cluster myetcd1=http://0.0.0.0:23801,myetcd2=http://0.0.0.0:23802,myetcd3=http://0.0.0.0:23803 查看member列表：
etcdctl --endpoints=http://127.0.0.1:23791 member list 47170e1b92eadb07: name=myetcd1 peerURLs=http://0.0.0.0:23801 clientURLs=http://0.0.0.0:23791 isLeader=true d150980031282a6b: name=myetcd2 peerURLs=http://0.0.0.0:23802 clientURLs=http://0.</description>
    </item>
    
  </channel>
</rss>